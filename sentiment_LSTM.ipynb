{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import time\n",
    "from IPython.core.display import clear_output\n",
    "from torch import autograd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./review_data.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "supa = df[df.label != 'unsup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "for label in supa.label:\n",
    "    labels.append(label)\n",
    "print(len(labels))\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Labels into a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = open(\"./review_data/labels.txt\", \"w\")\n",
    "for label in labels:\n",
    "    data_labels.write(label)\n",
    "    data_labels.write(\"\\n\")\n",
    "data_labels.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Once again Mr. Costner has dragged out a movie for far longer than necessary. Aside from the terrific sea rescue sequences, of which there are very few I just did not care about any of the characters. Most of us have ghosts in the closet, and Costner's character are realized early on, and then forgotten until much later, by which time I did not care. The character we should really care about is a very cocky, overconfident Ashton Kutcher. The problem is he comes off as kid who thinks he's better than anyone else around him and shows no signs of a cluttered closet. His only obstacle appears to be winning over Costner. Finally when we are well past the half way point of this stinker, Costner tells us all about Kutcher's ghosts. We are told why Kutcher is driven to be the best with no prior inkling or foreshadowing. No magic here, it was all I could do to keep from turning it off an hour in.\n"
     ]
    }
   ],
   "source": [
    "reviews = []\n",
    "for review in supa.review:\n",
    "    reviews.append(review)\n",
    "print(len(reviews))\n",
    "print(reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write reviews into text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reviews = open(\"./review_data/reviews.txt\", \"w\")\n",
    "for review in reviews:\n",
    "    data_reviews.write(review)\n",
    "    data_reviews.write(\"\\n\")\n",
    "data_reviews.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data From Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once again Mr. Costner has dragged out a movie for\n",
      "\n",
      "neg\n",
      "neg\n",
      "neg\n",
      "neg\n",
      "neg\n",
      "neg\n",
      "ne\n"
     ]
    }
   ],
   "source": [
    "with open(\"review_data/reviews.txt\", \"r\") as f:\n",
    "    reviews = f.read()\n",
    "with open(\"review_data/labels.txt\", \"r\") as f:\n",
    "    labels = f.read()\n",
    "\n",
    "print(reviews[:50])\n",
    "print()\n",
    "print(labels[:26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processesing - convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing — remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ''.join([c for c in reviews if c not in punctuation])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing — create list of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(all_text.split())\n",
    "word2id = {word: id+1 for id, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE EMBEDDINGS TO RANDOM VALUES\n",
    "embed_size = 300\n",
    "vocab_size = len(vocab)+1\n",
    "sd = 1/np.sqrt(embed_size)  # Standard deviation to use\n",
    "weights = np.random.normal(0, scale=sd, size=[vocab_size, embed_size])\n",
    "weights = weights.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"./review_data/glove.6B.300d.txt\"\n",
    "\n",
    "# EXTRACT DESIRED GLOVE WORD VECTORS FROM TEXT FILE\n",
    "with open(file, encoding=\"utf-8\", mode=\"r\") as textFile:\n",
    "    for line in textFile:\n",
    "        # Separate the values from the word\n",
    "        line = line.split()\n",
    "        word = line[0]\n",
    "\n",
    "        # If word is in our vocab, then update the corresponding weights\n",
    "        id = word2id.get(word, None)\n",
    "        if id is not None:\n",
    "            weights[id] = np.array(line[1:], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews : 50001\n",
      "Number of labels :50001\n"
     ]
    }
   ],
   "source": [
    "reviews_split = all_text.split('\\n')\n",
    "print ('Number of reviews :', len(reviews_split))\n",
    "labels_split = labels.split(\"\\n\")\n",
    "print('Number of labels :{}'.format(len(labels_split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize — Create Vocab to Int mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_text2 = ' '.join(reviews_split)\n",
    "# # create a list of words\n",
    "# words = all_text2.split()\n",
    "# # Count all the words using Counter Method\n",
    "# count_words = Counter(words)\n",
    "\n",
    "# total_words = len(words)\n",
    "# sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Total Words: \",total_words)\n",
    "# print(\"Word Count: \", len(count_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int = {w:i for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave Room for Padding for shorter reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocab_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize — Encode the words (replace words in reviews with integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[42446.0, 3096.0, 148885.0, 122577.0, 175763.0, 72798.0, 46946.0, 115161.0, 24806.0, 147409.0, 80423.0, 135373.0, 31413.0, 77860.0, 18990.0, 66234.0, 59999.0, 74755.0, 6231.0, 166710.0, 47456.0, 47949.0, 117081.0, 10353.0, 87605.0, 35986.0, 52984.0, 6738.0, 70944.0, 18429.0, 18403.0, 33888.0, 144428.0, 164225.0, 47949.0, 59999.0, 88404.0, 102786.0, 47949.0, 145987.0, 5718.0, 9257.0, 97114.0, 59999.0, 162786.0, 157378.0, 59664.0, 47500.0, 87605.0, 5592.0, 144332.0, 34890.0, 157378.0, 62462.0, 10923.0, 43503.0, 8676.0, 5630.0, 30994.0, 117081.0, 36578.0, 6738.0, 18429.0, 18403.0, 33888.0, 59999.0, 47500.0, 5844.0, 52104.0, 103817.0, 33888.0, 144428.0, 93925.0, 115161.0, 35986.0, 2372.0, 102411.0, 78435.0, 77708.0, 59999.0, 65463.0, 93925.0, 4137.0, 164538.0, 142154.0, 172667.0, 28494.0, 127768.0, 145375.0, 181117.0, 61980.0, 31413.0, 9351.0, 25358.0, 20797.0, 134430.0, 157378.0, 52137.0, 1576.0, 107580.0, 47949.0, 115161.0, 62225.0, 162786.0, 54054.0, 160647.0, 49281.0, 20954.0, 84560.0, 150074.0, 15222.0, 40116.0, 122577.0, 57316.0, 116795.0, 5844.0, 87605.0, 99989.0, 135142.0, 59999.0, 66245.0, 27161.0, 111306.0, 47949.0, 34722.0, 50207.0, 122577.0, 18918.0, 145987.0, 47813.0, 144428.0, 35158.0, 9257.0, 5844.0, 87605.0, 82678.0, 47541.0, 77708.0, 93925.0, 131382.0, 84560.0, 150074.0, 59999.0, 151682.0, 34566.0, 1576.0, 109167.0, 22810.0, 66653.0, 68959.0, 1576.0, 58039.0, 140562.0, 45123.0, 74858.0, 47813.0, 6738.0, 132713.0, 84914.0, 84560.0, 83871.0, 66234.0, 12456.0, 45123.0, 142154.0, 100922.0, 161024.0, 97114.0], [34722.0, 93925.0, 100922.0, 2410.0, 47949.0, 47541.0, 59999.0, 3552.0, 47949.0, 123893.0, 110471.0, 87605.0, 59999.0, 86896.0, 91881.0, 157378.0, 75753.0, 43716.0, 103817.0, 115404.0, 136519.0, 125962.0, 140562.0, 115161.0, 12439.0, 51882.0, 47949.0, 59999.0, 62462.0, 161896.0, 67567.0, 47949.0, 107864.0, 157378.0, 58742.0, 133089.0, 119500.0, 75427.0, 122053.0, 147144.0, 89036.0, 40116.0, 68894.0, 175336.0, 87605.0, 133641.0, 47949.0, 106055.0, 157378.0, 106055.0, 99989.0, 128837.0, 156545.0, 34566.0, 34722.0, 75454.0, 182324.0, 71626.0, 15470.0, 109290.0, 39678.0, 138064.0, 66653.0, 56987.0, 15470.0, 113402.0, 170518.0, 147409.0, 107864.0, 66653.0, 84954.0, 87722.0, 59999.0, 112069.0, 90881.0, 180364.0, 66653.0, 150728.0, 147409.0, 58742.0, 133089.0, 157378.0, 71626.0, 59999.0, 45199.0, 90016.0, 3802.0, 104670.0, 25143.0, 51506.0, 41896.0, 100506.0, 34722.0, 37221.0, 52477.0, 882.0, 59999.0, 179043.0, 157378.0, 153855.0, 15114.0, 172597.0, 28322.0, 59999.0, 122108.0, 166817.0, 115281.0, 74858.0, 156972.0, 97114.0, 34722.0, 37221.0, 157378.0, 47541.0, 59999.0, 122108.0, 11160.0, 4137.0, 67671.0, 26113.0, 59999.0, 77025.0, 86896.0, 47500.0, 66234.0, 90923.0, 180237.0, 36051.0, 37221.0, 131067.0, 100432.0, 34566.0, 166817.0, 115281.0, 175763.0, 134430.0, 153445.0, 59999.0, 77025.0, 86896.0, 116567.0, 47500.0, 157378.0, 882.0, 178662.0, 97114.0, 90923.0, 54054.0, 47500.0, 130910.0, 117081.0, 149723.0, 45123.0, 103477.0, 79879.0, 40500.0, 52103.0, 34722.0, 93925.0, 134731.0, 123893.0, 136254.0, 10353.0, 87605.0, 56669.0, 61980.0, 110471.0, 84560.0, 71626.0, 157378.0, 25357.0, 51850.0, 103817.0, 54983.0, 84560.0, 71626.0, 34722.0, 75454.0, 56987.0, 134287.0, 42623.0, 117081.0, 93925.0, 164423.0, 115161.0, 37078.0, 21331.0, 119295.0, 175763.0, 61980.0, 106055.0, 157378.0, 115161.0, 61980.0, 11155.0, 59999.0, 160647.0, 163828.0, 68894.0, 149723.0, 34722.0, 882.0, 47813.0, 136519.0, 125962.0, 74858.0, 115161.0, 157737.0, 4647.0, 34890.0, 59999.0, 145550.0, 59999.0, 142922.0, 74858.0, 44168.0, 163589.0, 117081.0, 164538.0, 151298.0, 84560.0, 123330.0, 122000.0, 147409.0, 59999.0, 175387.0, 37221.0, 110821.0, 119295.0, 18403.0, 165042.0, 140161.0], [120910.0, 47949.0, 47813.0, 6738.0, 182231.0, 59710.0, 116390.0, 69241.0, 127768.0, 175563.0, 104876.0, 25357.0, 175336.0, 6326.0, 115161.0, 145713.0, 118570.0, 174880.0, 59244.0, 39468.0, 47813.0, 175336.0, 84914.0, 93925.0, 26451.0, 157378.0, 108978.0, 75427.0, 166017.0, 157378.0, 106055.0, 72107.0, 59323.0, 114519.0, 47949.0, 63872.0, 40500.0, 59999.0, 24806.0, 19699.0, 91569.0, 110263.0, 31413.0, 38037.0, 111201.0, 84560.0, 172361.0, 28322.0, 93925.0, 1220.0, 34890.0, 22330.0, 38498.0, 10819.0, 882.0, 59999.0, 119991.0, 10353.0, 93925.0, 18403.0, 115161.0, 88675.0, 89111.0, 47500.0, 97114.0, 34722.0, 24806.0, 175350.0, 147409.0, 59999.0, 77136.0, 27593.0, 127768.0, 93925.0, 120122.0, 59999.0, 160647.0, 75454.0, 34566.0, 66245.0, 115161.0, 6717.0, 40500.0, 166817.0, 115281.0, 157378.0, 56077.0, 169295.0, 87605.0, 27350.0, 174094.0, 128258.0, 157378.0, 99439.0, 47500.0, 93925.0, 70944.0, 172667.0, 8676.0, 115161.0, 38213.0, 172667.0, 59999.0, 70130.0, 6738.0, 96221.0, 72107.0, 134430.0, 3639.0, 66234.0, 59999.0, 116436.0, 40500.0, 59999.0, 24806.0, 93925.0, 90865.0, 34566.0, 137583.0, 30810.0, 157378.0, 157167.0, 131124.0, 116453.0, 4085.0, 20494.0, 126077.0, 173905.0, 34566.0, 77409.0, 139082.0, 43225.0, 10353.0, 93925.0, 60389.0, 8676.0, 1576.0, 109052.0, 157378.0, 45123.0, 93925.0, 115161.0, 139766.0, 65463.0, 116795.0, 51850.0, 155912.0, 147409.0, 136337.0, 168026.0, 114829.0, 175350.0, 66234.0, 115281.0, 157378.0, 59999.0, 77136.0, 27593.0, 157378.0, 168026.0, 21728.0, 28322.0, 175336.0, 91037.0, 40500.0, 59999.0, 160647.0, 15227.0, 121843.0, 4085.0, 68894.0, 66989.0, 104876.0, 93925.0, 59999.0, 77136.0, 27593.0, 157378.0, 59999.0, 4788.0, 119295.0, 31609.0, 24114.0, 30994.0, 172332.0, 18403.0, 156661.0, 125058.0, 79562.0, 150416.0, 70290.0, 40500.0, 61429.0, 107159.0, 66234.0, 34722.0, 114928.0, 157378.0, 56987.0, 45049.0, 75751.0, 114402.0, 157378.0, 25676.0, 164871.0, 882.0, 52588.0, 175336.0, 5718.0, 88404.0, 51850.0, 33888.0, 144428.0, 115161.0, 28879.0, 47949.0, 91272.0, 157378.0, 115404.0, 119295.0, 45199.0, 24114.0, 97114.0, 59999.0, 82385.0]]\n"
     ]
    }
   ],
   "source": [
    "reviews_int = []\n",
    "for review in reviews_split:\n",
    "    r = [float(word2id[w]) for w in review.split()]\n",
    "    reviews_int.append(r)\n",
    "print (reviews_int[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Tokenize — Encode the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = [1.0 if label =='pos' else 0.0 for label in labels_split]\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Reviews Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X-axis : Review Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y-axis: NUmber of Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFu5JREFUeJzt3X+s3XWd5/Hnawo4xB9LEb1pgGyZnf4hSgbxBpq4mdzFCRT8o5hoAiFDV0k6ayCrCbuxziSLI5LoJmgCUZIau5YJKxJ/pI3W7TQMJ8ZEfipSKsP0il2pdCFuEbmaxa373j/Op5Mz/Z7be+/p7b29Pc9HcnK+5/39fL/n+z6nt6/7/XHOTVUhSdKgP1ruDZAknXoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6zljuDRjVeeedV2vXrh1p2d/+9re88Y1vXNwNWgHGse9x7BnGs+9x7BkW3veTTz75q6p621zjVmw4rF27lieeeGKkZXu9HlNTU4u7QSvAOPY9jj3DePY9jj3DwvtO8j/nM87DSpKkDsNBktRhOEiSOgwHSVKH4SBJ6pgzHJL8cZLHkvwkyb4kf9vqX03y8yRPtdulrZ4kdyeZTvJ0kssG1rUpyf522zRQf0+SvW2Zu5PkZDQrSZqf+VzK+jpwZVXNJDkT+EGS77V5/7mqvnHM+GuAde12BXAvcEWSc4HbgUmggCeT7KyqV9qYzcAjwC5gA/A9JEnLYs49h+qbaQ/PbLfj/W3RjcB9bblHgHOSrAGuBvZU1eEWCHuADW3eW6rqh9X/m6X3AdedQE+SpBM0rw/BJVkFPAn8KfDFqno0yUeBO5P8F+AhYEtVvQ6cD7wwsPjBVjte/eCQ+rDt2Ex/D4OJiQl6vd58Nr9jZmZm5GVXsnHsexx7hvHsexx7hpPX97zCoar+AFya5Bzg20neBXwS+F/AWcBW4BPAp4Fh5wtqhPqw7djanovJycka9dOQ99y/g7t+8NuRlj0RBz77/iV/zkHj+AnScewZxrPvcewZTl7fC7paqap+DfSADVV1qB06eh34b8DlbdhB4MKBxS4AXpyjfsGQuiRpmcznaqW3tT0GkpwN/AXwj+1cAe3KouuAZ9oiO4Gb2lVL64FXq+oQsBu4KsnqJKuBq4Ddbd5rSda3dd0E7FjcNiVJCzGfw0prgO3tvMMfAQ9W1XeS/EOSt9E/LPQU8B/a+F3AtcA08DvgwwBVdTjJHcDjbdynq+pwm/4o8FXgbPpXKXmlkiQtoznDoaqeBt49pH7lLOMLuGWWeduAbUPqTwDvmmtbJElLw09IS5I6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHXOGQ5I/TvJYkp8k2Zfkb1v9oiSPJtmf5OtJzmr1N7TH023+2oF1fbLVn0ty9UB9Q6tNJ9my+G1KkhZiPnsOrwNXVtWfAZcCG5KsBz4HfKGq1gGvADe38TcDr1TVnwJfaONIcjFwPfBOYAPwpSSrkqwCvghcA1wM3NDGSpKWyZzhUH0z7eGZ7VbAlcA3Wn07cF2b3tge0+a/L0la/YGqer2qfg5MA5e323RVPV9VvwceaGMlSctkXucc2m/4TwEvA3uAnwG/rqojbchB4Pw2fT7wAkCb/yrw1sH6McvMVpckLZMz5jOoqv4AXJrkHODbwDuGDWv3mWXebPVhAVVDaiTZDGwGmJiYoNfrHX/DZzFxNtx2yZG5By6yUbd3sczMzCz7Niy1cewZxrPvcewZTl7f8wqHo6rq10l6wHrgnCRntL2DC4AX27CDwIXAwSRnAP8KODxQP2pwmdnqxz7/VmArwOTkZE1NTS1k8//ZPffv4K69C2p9URy4cWrJn3NQr9dj1NdspRrHnmE8+x7HnuHk9T2fq5Xe1vYYSHI28BfAs8DDwAfbsE3Ajja9sz2mzf+HqqpWv75dzXQRsA54DHgcWNeufjqL/knrnYvRnCRpNPP59XkNsL1dVfRHwINV9Z0kPwUeSPIZ4MfAV9r4rwB/l2Sa/h7D9QBVtS/Jg8BPgSPALe1wFUluBXYDq4BtVbVv0TqUJC3YnOFQVU8D7x5Sf57+lUbH1v8P8KFZ1nUncOeQ+i5g1zy2V5K0BPyEtCSpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI65gyHJBcmeTjJs0n2JflYq38qyS+TPNVu1w4s88kk00meS3L1QH1Dq00n2TJQvyjJo0n2J/l6krMWu1FJ0vzNZ8/hCHBbVb0DWA/ckuTiNu8LVXVpu+0CaPOuB94JbAC+lGRVklXAF4FrgIuBGwbW87m2rnXAK8DNi9SfJGkEc4ZDVR2qqh+16deAZ4Hzj7PIRuCBqnq9qn4OTAOXt9t0VT1fVb8HHgA2JglwJfCNtvx24LpRG5IknbgFnXNIshZ4N/BoK92a5Okk25KsbrXzgRcGFjvYarPV3wr8uqqOHFOXJC2TM+Y7MMmbgG8CH6+q3yS5F7gDqHZ/F/ARIEMWL4YHUR1n/LBt2AxsBpiYmKDX68138/+FibPhtkuOzD1wkY26vYtlZmZm2bdhqY1jzzCefY9jz3Dy+p5XOCQ5k34w3F9V3wKoqpcG5n8Z+E57eBC4cGDxC4AX2/Sw+q+Ac5Kc0fYeBsf/C1W1FdgKMDk5WVNTU/PZ/I577t/BXXvnnYuL5sCNU0v+nIN6vR6jvmYr1Tj2DOPZ9zj2DCev7/lcrRTgK8CzVfX5gfqagWEfAJ5p0zuB65O8IclFwDrgMeBxYF27Muks+ietd1ZVAQ8DH2zLbwJ2nFhbkqQTMZ9fn98L/CWwN8lTrfbX9K82upT+IaADwF8BVNW+JA8CP6V/pdMtVfUHgCS3AruBVcC2qtrX1vcJ4IEknwF+TD+MJEnLZM5wqKofMPy8wK7jLHMncOeQ+q5hy1XV8/SvZpIknQL8hLQkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOuYMhyQXJnk4ybNJ9iX5WKufm2RPkv3tfnWrJ8ndSaaTPJ3ksoF1bWrj9yfZNFB/T5K9bZm7k+RkNCtJmp/57DkcAW6rqncA64FbklwMbAEeqqp1wEPtMcA1wLp22wzcC/0wAW4HrgAuB24/GihtzOaB5TaceGuSpFHNGQ5VdaiqftSmXwOeBc4HNgLb27DtwHVteiNwX/U9ApyTZA1wNbCnqg5X1SvAHmBDm/eWqvphVRVw38C6JEnL4IyFDE6yFng38CgwUVWHoB8gSd7ehp0PvDCw2MFWO1794JD6sOffTH8Pg4mJCXq93kI2/59NnA23XXJkpGVPxKjbu1hmZmaWfRuW2jj2DOPZ9zj2DCev73mHQ5I3Ad8EPl5VvznOaYFhM2qEerdYtRXYCjA5OVlTU1NzbPVw99y/g7v2LigXF8WBG6eW/DkH9Xo9Rn3NVqpx7BnGs+9x7BlOXt/zulopyZn0g+H+qvpWK7/UDgnR7l9u9YPAhQOLXwC8OEf9giF1SdIymc/VSgG+AjxbVZ8fmLUTOHrF0SZgx0D9pnbV0nrg1Xb4aTdwVZLV7UT0VcDuNu+1JOvbc900sC5J0jKYz7GV9wJ/CexN8lSr/TXwWeDBJDcDvwA+1ObtAq4FpoHfAR8GqKrDSe4AHm/jPl1Vh9v0R4GvAmcD32s3SdIymTMcquoHDD8vAPC+IeMLuGWWdW0Dtg2pPwG8a65tkSQtDT8hLUnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdcwZDkm2JXk5yTMDtU8l+WWSp9rt2oF5n0wyneS5JFcP1De02nSSLQP1i5I8mmR/kq8nOWsxG5QkLdx89hy+CmwYUv9CVV3abrsAklwMXA+8sy3zpSSrkqwCvghcA1wM3NDGAnyurWsd8Apw84k0JEk6cXOGQ1V9Hzg8z/VtBB6oqter6ufANHB5u01X1fNV9XvgAWBjkgBXAt9oy28HrltgD5KkRXYi5xxuTfJ0O+y0utXOB14YGHOw1WarvxX4dVUdOaYuSVpGZ4y43L3AHUC1+7uAjwAZMrYYHkJ1nPFDJdkMbAaYmJig1+staKOPmjgbbrvkyNwDF9mo27tYZmZmln0blto49gzj2fc49gwnr++RwqGqXjo6neTLwHfaw4PAhQNDLwBebNPD6r8CzklyRtt7GBw/7Hm3AlsBJicna2pqapTN5577d3DX3lFzcXQHbpxa8ucc1Ov1GPU1W6nGsWcYz77HsWc4eX2PdFgpyZqBhx8Ajl7JtBO4PskbklwErAMeAx4H1rUrk86if9J6Z1UV8DDwwbb8JmDHKNskSVo8c/76nORrwBRwXpKDwO3AVJJL6R8COgD8FUBV7UvyIPBT4AhwS1X9oa3nVmA3sArYVlX72lN8AnggyWeAHwNfWbTuJEkjmTMcquqGIeVZ/wOvqjuBO4fUdwG7htSfp381kyTpFOEnpCVJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqWPOPxOqxbN2y3eX7bkPfPb9y/bcklYe9xwkSR2GgySpY85wSLItyctJnhmonZtkT5L97X51qyfJ3Ummkzyd5LKBZTa18fuTbBqovyfJ3rbM3Umy2E1KkhZmPnsOXwU2HFPbAjxUVeuAh9pjgGuAde22GbgX+mEC3A5cAVwO3H40UNqYzQPLHftckqQlNmc4VNX3gcPHlDcC29v0duC6gfp91fcIcE6SNcDVwJ6qOlxVrwB7gA1t3luq6odVVcB9A+uSJC2TUc85TFTVIYB2//ZWPx94YWDcwVY7Xv3gkLokaRkt9qWsw84X1Aj14StPNtM/BMXExAS9Xm+ETYSJs+G2S46MtOxK1ev1mJmZGfk1W6nGsWcYz77HsWc4eX2PGg4vJVlTVYfaoaGXW/0gcOHAuAuAF1t96ph6r9UvGDJ+qKraCmwFmJycrKmpqdmGHtc99+/grr3j9RGPAzdO0ev1GPU1W6nGsWcYz77HsWc4eX2PelhpJ3D0iqNNwI6B+k3tqqX1wKvtsNNu4Kokq9uJ6KuA3W3ea0nWt6uUbhpYlyRpmcz563OSr9H/rf+8JAfpX3X0WeDBJDcDvwA+1IbvAq4FpoHfAR8GqKrDSe4AHm/jPl1VR09yf5T+FVFnA99rN0nSMpozHKrqhllmvW/I2AJumWU924BtQ+pPAO+aazskSUvHT0hLkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jAcJEkdJxQOSQ4k2ZvkqSRPtNq5SfYk2d/uV7d6ktydZDrJ00kuG1jPpjZ+f5JNJ9aSJOlELcaew7+rqkurarI93gI8VFXrgIfaY4BrgHXtthm4F/phAtwOXAFcDtx+NFAkScvjZBxW2ghsb9PbgesG6vdV3yPAOUnWAFcDe6rqcFW9AuwBNpyE7ZIkzdOJhkMBf5/kySSbW22iqg4BtPu3t/r5wAsDyx5stdnqkqRlcsYJLv/eqnoxyduBPUn+8ThjM6RWx6l3V9APoM0AExMT9Hq9BW5u38TZcNslR0ZadqXq9XrMzMyM/JqtVOPYM4xn3+PYM5y8vk8oHKrqxXb/cpJv0z9n8FKSNVV1qB02erkNPwhcOLD4BcCLrT51TL03y/NtBbYCTE5O1tTU1LBhc7rn/h3ctfdEc3FlOXDjFL1ej1Ffs5VqHHuG8ex7HHuGk9f3yIeVkrwxyZuPTgNXAc8AO4GjVxxtAna06Z3ATe2qpfXAq+2w027gqiSr24noq1pNkrRMTuTX5wng20mOrue/V9X/SPI48GCSm4FfAB9q43cB1wLTwO+ADwNU1eEkdwCPt3GfrqrDJ7BdkqQTNHI4VNXzwJ8Nqf9v4H1D6gXcMsu6tgHbRt0WSdLi8hPSkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHWM1xcMjbG1W77LbZcc4d9v+e6SPu+Bz75/SZ9P0uJwz0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOvz6DJ1Ua5f46zoG+dUd0ujcc5AkdZwy4ZBkQ5Lnkkwn2bLc2yNJ4+yUCIckq4AvAtcAFwM3JLl4ebdKksbXKREOwOXAdFU9X1W/Bx4ANi7zNknS2DpVTkifD7ww8PggcMUybYtOE/4NC2l0p0o4ZEitOoOSzcDm9nAmyXMjPt95wK9GXHbF+o9j2Pdy9JzPLeWzzWrs3mvGs2dYeN//ej6DTpVwOAhcOPD4AuDFYwdV1VZg64k+WZInqmryRNez0oxj3+PYM4xn3+PYM5y8vk+Vcw6PA+uSXJTkLOB6YOcyb5Mkja1TYs+hqo4kuRXYDawCtlXVvmXeLEkaW6dEOABU1S5g1xI93QkfmlqhxrHvcewZxrPvcewZTlLfqeqc95UkjblT5ZyDJOkUMlbhcLp/RUeSA0n2JnkqyROtdm6SPUn2t/vVrZ4kd7fX4ukkly3v1s9fkm1JXk7yzEBtwX0m2dTG70+yaTl6ma9Zev5Ukl+29/upJNcOzPtk6/m5JFcP1FfUz0CSC5M8nOTZJPuSfKzVT9v3+zg9L+37XVVjcaN/ovtnwJ8AZwE/AS5e7u1a5B4PAOcdU/uvwJY2vQX4XJu+Fvge/c+YrAceXe7tX0Cffw5cBjwzap/AucDz7X51m1693L0tsOdPAf9pyNiL27/vNwAXtX/3q1bizwCwBrisTb8Z+KfW32n7fh+n5yV9v8dpz2Fcv6JjI7C9TW8Hrhuo31d9jwDnJFmzHBu4UFX1feDwMeWF9nk1sKeqDlfVK8AeYMPJ3/rRzNLzbDYCD1TV61X1c2Ca/r//FfczUFWHqupHbfo14Fn636hw2r7fx+l5Nifl/R6ncBj2FR3He8FXogL+PsmT7dPkABNVdQj6/+iAt7f66fZ6LLTP06X/W9vhk21HD61wmvacZC3wbuBRxuT9PqZnWML3e5zCYV5f0bHCvbeqLqP/7ba3JPnz44wdh9cDZu/zdOj/XuDfAJcCh4C7Wv206znJm4BvAh+vqt8cb+iQ2orsfUjPS/p+j1M4zOsrOlayqnqx3b8MfJv+buVLRw8XtfuX2/DT7fVYaJ8rvv+qeqmq/lBV/w/4Mv33G06znpOcSf8/yfur6lutfFq/38N6Xur3e5zC4bT+io4kb0zy5qPTwFXAM/R7PHplxiZgR5veCdzUru5YD7x6dDd9hVpon7uBq5KsbrvnV7XainHMOaIP0H+/od/z9UnekOQiYB3wGCvwZyBJgK8Az1bV5wdmnbbv92w9L/n7vdxn5pfyRv9Khn+ifwb/b5Z7exa5tz+hfzXCT4B9R/sD3go8BOxv9+e2euj/gaWfAXuByeXuYQG9fo3+bvX/pf/b0c2j9Al8hP7Ju2ngw8vd1wg9/13r6en2Q79mYPzftJ6fA64ZqK+onwHg39I/FPI08FS7XXs6v9/H6XlJ328/IS1J6hinw0qSpHkyHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUsf/B7dleXCBeDgvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    50001.000000\n",
       "mean       230.249895\n",
       "std        170.661355\n",
       "min          0.000000\n",
       "25%        126.000000\n",
       "50%        172.000000\n",
       "75%        280.000000\n",
       "max       2469.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_len = [len(x) for x in reviews_int]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Outliers — Getting rid of extremely long or short reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations : a) Mean review length = 240 b) Some reviews are of 0 length. Keeping this review won’t make any sense for our analysis c) Most of the reviews less than 500 words or more d) There are quite a few reviews that are extremely long, we can manually investigate them to check whether we need to include or exclude them from our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding / Truncating the remaining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deal with both short and long reviews, we will pad or truncate all our reviews to a specific length - Sequence Length - tje same as number of time steps for LSTM layer.\n",
    "\n",
    "For reviews shorter than seq_length, we will pad with 0s. For reviews longer than seq_length we will truncate them to the first seq_length words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' \n",
    "        Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = float)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features\n",
    "\n",
    "features = pad_features(reviews_int, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Length:  50000\n",
      "Labels Length:  50000\n",
      "Positive Features:  25000\n",
      "Negative Features:  25000\n",
      "Positive Labels:  25000\n",
      "Negative Labels:  25000\n"
     ]
    }
   ],
   "source": [
    "print(\"Features Length: \", len(features))\n",
    "print(\"Labels Length: \", len(encoded_labels))\n",
    "features1 = features[0:12500]\n",
    "features2 = features[12500:25000]\n",
    "features3 = features[25000:37500]\n",
    "features4 = features[37500:50000]\n",
    "features_pos = np.concatenate((features2, features4))\n",
    "features_neg = np.concatenate((features1, features3))\n",
    "print(\"Positive Features: \", len(features_pos))\n",
    "print(\"Negative Features: \", len(features_neg))\n",
    "labels1 = encoded_labels[0:12500]\n",
    "labels2 = encoded_labels[12500:25000]\n",
    "labels3 = encoded_labels[25000:37500]\n",
    "labels4 = encoded_labels[37500:50000]\n",
    "labels_pos = labels2 + labels4\n",
    "labels_neg = labels1 + labels3\n",
    "print(\"Positive Labels: \", len(labels_pos))\n",
    "print(\"Negative Labels: \", len(labels_neg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0.      0.      0. ... 100922. 161024.  97114.]\n",
      " [ 34722.  93925. 100922. ... 163828.  68894. 149723.]\n",
      " [120910.  47949.  47813. ... 157378.  56987.  45049.]\n",
      " ...\n",
      " [  6738. 153802. 150377. ...   4137. 112884.  34890.]\n",
      " [     0.      0.      0. ... 172851. 172667.  99989.]\n",
      " [     0.      0.      0. ... 166017.  27161.  66234.]]\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[     0.      0.      0. ... 100922. 161024.  97114.]\n",
      " [ 34722.  93925. 100922. ... 163828.  68894. 149723.]\n",
      " [120910.  47949.  47813. ... 157378.  56987.  45049.]\n",
      " ...\n",
      " [     0.      0.      0. ...  64253.  40500. 162312.]\n",
      " [     0.      0.      0. ...   8401.  95282.  49105.]\n",
      " [     0.      0.      0. ... 127598.  10399.  66234.]]\n"
     ]
    }
   ],
   "source": [
    "print(features[:10,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, Validation, Test Dataset Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train= 80% | valid = 10% | test = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "# split_frac = 0.5\n",
    "# len_feat = len(features)\n",
    "# train_x = features[0:int(split_frac*len_feat)]\n",
    "# train_x = np.asarray(train_x, dtype=np.float32)\n",
    "# train_y = encoded_labels[0:int(split_frac*len_feat)]\n",
    "# train_y = np.asarray(train_y, dtype=np.float32)\n",
    "# remaining_x = features[int(split_frac*len_feat):]\n",
    "# remaining_x = np.asarray(remaining_x, dtype=np.float32)\n",
    "# remaining_y = encoded_labels[int(split_frac*len_feat):]\n",
    "# remaining_y = np.asarray(remaining_y, dtype=np.float32)\n",
    "# valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "# valid_x = np.asarray(valid_x, dtype=np.float32)\n",
    "# valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "# valid_y = np.asarray(valid_y, dtype=np.float32)\n",
    "# test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "# text_x = np.asarray(test_x, dtype=np.float32)\n",
    "# test_y = remaining_y[int(len(remaining_y)*0.5):]\n",
    "# test_y = np.asarray(test_y, dtype=np.float32)\n",
    "\n",
    "\n",
    "\n",
    "train_percent = .8\n",
    "print(int(len(features_pos)*train_percent))\n",
    "train_x = np.concatenate((features_pos[0:int(len(features_pos)*train_percent)], features_neg[0:int(len(features_neg)*train_percent)]))\n",
    "train_x = np.asarray(train_x, dtype=np.float32)\n",
    "train_y = labels_pos[0:int(len(labels_pos)*train_percent)] + labels_neg[0:int(len(labels_neg)*train_percent)]\n",
    "train_y = np.asarray(train_y, dtype=np.float32)\n",
    "\n",
    "features_pos = features_pos[int(len(features_pos)*train_percent):]\n",
    "features_neg = features_neg[int(len(features_neg)*train_percent):]\n",
    "labels_pos = labels_pos[int(len(labels_pos)*train_percent):]\n",
    "labels_neg = labels_neg[int(len(labels_neg)*train_percent):]\n",
    "\n",
    "valid_x = np.concatenate((features_pos[0:int(len(features_pos)*.5)], features_neg[0:int(len(features_neg)*.5)]))\n",
    "valid_x = np.asarray(valid_x, dtype=np.float32)\n",
    "valid_y = labels_pos[0:int(len(labels_pos)*.5)] + labels_neg[0:int(len(labels_neg)*.5)]\n",
    "valid_y = np.asarray(valid_y, dtype=np.float32)\n",
    "\n",
    "features_pos = features_pos[int(len(features_pos)*.5):]\n",
    "features_neg = features_neg[int(len(features_neg)*.5):]\n",
    "labels_pos = labels_pos[int(len(labels_pos)*.5):]\n",
    "labels_neg = labels_neg[int(len(labels_neg)*.5):]\n",
    "\n",
    "test_x = np.concatenate((features_pos, features_neg))\n",
    "test_x = np.asarray(test_x, dtype=np.float32)\n",
    "test_y = labels_pos + labels_neg\n",
    "test_y = np.asarray(test_y, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataloaders and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain one batch of training data for visualization purpose we will create a data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Size = 50;\n",
    "Sequence Length = 200;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the LSTM Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](files/network_arch.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "# vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "vocab_size = len(vocab)+1\n",
    "output_size = 1\n",
    "embedding_dim = 300\n",
    "# embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_layers = 4\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "#UPDATING PYTORCH EMBEDDINGS\n",
    "net.embedding.weight.data = torch.Tensor(weights)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 12 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "train_on_gpu = True\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "\n",
    "t_loss = []\n",
    "v_loss = []\n",
    "plt.figure()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        inputs = inputs.cuda()\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                inputs = inputs.cuda()\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            \"\"\"\n",
    "                PLOT EPOCHS IN GRAPH\n",
    "            \"\"\"\n",
    "            t_loss.append(loss.item())\n",
    "            v_loss.append(np.mean(val_losses))\n",
    "            plt.plot(t_loss)\n",
    "            plt.plot(v_loss)\n",
    "            plt.show()\n",
    "            clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    inputs = inputs.cuda()\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([word2id[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "\n",
    "test_review_neg = \"Awful movie. Never will I ever recommend it to anyone.\"\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(test_review_neg)\n",
    "print(test_ints)\n",
    "\n",
    "\n",
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)\n",
    "\n",
    "\n",
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = 'This movie had the best acting and the dialogue was so good. I loved it. Great movie'\n",
    "seq_length=200 # good to use the length that was trained on\n",
    "predict(net, test_review_neg, seq_length)\n",
    "predict(net, test_review, seq_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
